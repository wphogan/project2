{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from data_handler import *\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, emb_size, nhead, nhid, nlayers):\n",
    "        \"\"\"\n",
    "        emb_size: Embedding Size for the input\n",
    "        ntoken: Number of tokens Vocab Size\n",
    "        nhead: Number of transformer heads in the encoder\n",
    "        nhid: Number of hidden units in transformer encoder layer\n",
    "        nlayer: Number of layers in transformer encoder\n",
    "        \"\"\"\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        \n",
    "        \"\"\"\n",
    "        1. Initialize position input embedding, position encoding layers\n",
    "        2. Initialize transformer encoder with nlayers and each layer \n",
    "        having nhead heads and nhid hidden units.\n",
    "        3. Decoder can be implemented directly on top of the encoder as a linear layer. \n",
    "           To keep things simple, we are predicting one token for each of the input tokens. \n",
    "           We can pad the input to have the same length as target to ensure we can generate all target tokens. \n",
    "           You may experiment with a transformer decoder and use teacher forcing during training, \n",
    "           but it is not necessary to do so.\n",
    "        \"\"\"\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        src: tensor of shape (seq_len, batch_size)\n",
    "        \n",
    "        Returns:\n",
    "            output: tensor of shape (seq_len, batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        1. Embed the source sequences and add the positional encoding.\n",
    "        2. Pass the sequence to the transformer encoder\n",
    "        3. Generate and return scores using decoder linear layer\n",
    "        \"\"\"\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds positional embedding to the input for conditioning on time. \n",
    "    This is already implemented for you, but you can try other variants of positional encoding.\n",
    "    Read the paper \"Attention is all you need\" for more details on this. \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: tensor of shape (seq_len, batch_size, embedding_size)\n",
    "        Returns:\n",
    "            x: tensor of shape (seq_len, batch_size, embedding_size)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Sequences torch.Size([5109, 24]) torch.Size([5109, 24])\n",
      "Val Sequences torch.Size([1278, 24]) torch.Size([1278, 24])\n"
     ]
    }
   ],
   "source": [
    "line_pairs, vocab_size, idx_dict = load_data()\n",
    "\n",
    "char_to_index = idx_dict['char_to_index']\n",
    "start_token = idx_dict['start_token']\n",
    "end_token = idx_dict['end_token']\n",
    "\n",
    "num_lines = len(line_pairs)\n",
    "num_train = int(0.8 * num_lines)\n",
    "train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
    "\n",
    "# Create source and target tensors for the pairs: \n",
    "# Shape of source (NumExamples, MaxSentLength), Shape of target (NumExamples, MaxSentLength). \n",
    "\n",
    "# Make sure source and targets have the same shape by padding them to the same length with end tokens. \n",
    "# This is because our transformer implementation predicts one token for each input token\n",
    "# During inference for a single example, we can sufficiently pad the input with end tokens.\n",
    "\n",
    "def convert_dataset_to_tensor(data_pairs, max_len):\n",
    "    source_rows = []\n",
    "    target_rows = []\n",
    "    for pair in data_pairs:\n",
    "        source, target = pair\n",
    "        \n",
    "        source_index_list = string_to_index_list(source, char_to_index, end_token)\n",
    "        source_index_list = source_index_list + [end_token for et in range(max_len - (len(source_index_list)))]\n",
    "        \n",
    "        target_index_list = [start_token] + string_to_index_list(target, char_to_index, end_token)\n",
    "        target_index_list = target_index_list + [end_token for et in range(max_len - (len(target_index_list)))]\n",
    "        \n",
    "        source_rows.append( torch.LongTensor(source_index_list) )\n",
    "        target_rows.append( torch.LongTensor(target_index_list) )\n",
    "        \n",
    "    source_tensors = torch.stack(source_rows)\n",
    "    target_tensors = torch.stack(target_rows)\n",
    "\n",
    "    return source_tensors, target_tensors\n",
    "\n",
    "line_pairs, vocab_size, idx_dict = load_data()\n",
    "\n",
    "char_to_index = idx_dict['char_to_index']\n",
    "start_token = idx_dict['start_token']\n",
    "end_token = idx_dict['end_token']\n",
    "\n",
    "num_lines = len(line_pairs)\n",
    "num_train = int(0.8 * num_lines)\n",
    "train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
    "\n",
    "source_strings = [pair[0] for pair in line_pairs]\n",
    "target_strings = [pair[1] for pair in line_pairs]\n",
    "\n",
    "max_input_len = max([ len(source_string)+1 for source_string in source_strings])\n",
    "max_target_len = max([ len(target_string)+2 for target_string in target_strings])\n",
    "max_len = max(max_input_len, max_target_len)\n",
    "\n",
    "\n",
    "\n",
    "train_inputs, train_targets = convert_dataset_to_tensor(train_pairs, max_len)\n",
    "val_inputs, val_targets = convert_dataset_to_tensor(val_pairs, max_len)\n",
    "\n",
    "print (\"Train Sequences\", train_inputs.size(), train_targets.size())\n",
    "print (\"Val Sequences\", val_inputs.size(), val_targets.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ntokens = vocab_size # the size of vocabulary\n",
    "batch_size = 16\n",
    "emsize = 50 # embedding dimension\n",
    "nhid = 50 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "lr = 1.0 # learning rate\n",
    "epochs = 50 # The number of epochs\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# Implement Training Loop Here\n",
    "for epoch in range(epochs):\n",
    "    # num_batches = ... \n",
    "    for batch_no in range(num_batches):\n",
    "        # input, target = get_batch(batch_no, .. )  \n",
    "        # Process input to the model, get the ouput, compute loss\n",
    "        # backpropagate loss and update weights\n",
    "        # Calculate avg training loss over the batches.\n",
    "    \n",
    "    # val_loss, val_acc = evaluate(model, val_data)\n",
    "    # sample_translation = translate(model, \"testsequence\")\n",
    "    # print (\"Epoch:{} | Train Loss:{} | Val Loss:{} | Val Acc:{} \".format(epoch, train_loss, val_loss, val_acc))\n",
    "    # print (sample_translation)\n",
    "    # model.train() \n",
    "    # scheduler.step()\n",
    "\n",
    "def evaluate(model, val_data):\n",
    "    # Feel free the change the arguments this function accepts\n",
    "    # model.eval() # Turn on the evaluation mode\n",
    "    \n",
    "    # Return validation loss and\n",
    "    # Accuracy -> percentage of validation sequences that were translated correctly.\n",
    "    \n",
    "    # eval_model.train() # Turn off the evaluation mode, turn on training mode\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def translate(model, input_sequence):\n",
    "    # Translates the input sequence to piglatin using the trained model\n",
    "    # model.eval()\n",
    "        \n",
    "    # Convert input_sequence to a tensor of appropriate shape,\n",
    "    # process it through the model and predict the translation.\n",
    "    \n",
    "    # model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
